{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align='center'>Ética en Inteligencia Artificial</h1>\n",
    "\n",
    "<h2 align='center'>Introducción a Fairlearn</h2>\n",
    "\n",
    "<h3 align='center'>Laura G. Funderburk</h3>\n",
    "\n",
    "<h3 align='center'>Data Scientist, Cybera</h3> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Motivación</h2>\n",
    "\n",
    "\n",
    "En los últimos años, hemos visto un aumento en el uso de la inteligencia artificial en nuestra vida cotidiana: desde algoritmos de aprendizaje automático que revisan los currículums para las ofertas de trabajo, hasta sistemas que recomiendan tratamiento a pacientes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Motivación</h2>\n",
    "\n",
    "¿Cuál es nuestra responsabilidad como científicos de datos, ingenieros de datos, programadores y colectores de datos?\n",
    "\n",
    "¿Qué podemos hacer para identificar áreas de prejucio en nuestros modelos y minimizar daño?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Motivación</h2>\n",
    "\n",
    "\n",
    "En este tutorial, los participantes explorarán las trampas de la inteligencia artificial cuando se trata de algoritmos responsables de tomar decisiones que impactan vidas y qué podemos hacer para mitigar la injusticia de nuestros modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Plan para el tutorial</h2>\n",
    "\n",
    "* Teoría\n",
    "\n",
    "    1. Motivar la importancia de la ética en la inteligencia artificial (IA).\n",
    "    2. Cómo determinar la fuente de injusticia en la IA.\n",
    "    3. El papel de la abstracción en la IA.\n",
    "    4. Marcos de referencia en la abstracción del aprendizaje automático.\n",
    "    5. Trampas en las que podemos caer al abstraernos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Plan para el tutorial</h2>\n",
    "\n",
    "\n",
    "* Práctica\n",
    "\n",
    "    1. Introducción a Fairlearn.\n",
    "    2. Ejercicio de práctica en el sector médico.\n",
    "    3. Identificar métricas de equidad.\n",
    "    4. Resumen estadístico. \n",
    "    5. Modelo de aprendizaje automático: división, entrenamiento y predicción de datos.\n",
    "    5. Evaluar precisión y equidad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Motivación</h2>\n",
    "\n",
    "Considere la evaluación del riesgo de recaída en el comportamiento delincuente y recomendaciones sobre las medidas adecuadas para prevenir la recaída.\n",
    "\n",
    "\n",
    "**Reto:**  desarrollar un modelo que tome como entrada información sobre el caso y, como resultado, devuelva una puntuación que se utilizará para determinar un curso de acción apropiado.\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/handcuff.jpeg\" width=\"500\"/>|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Motivación</h2>\n",
    "\n",
    "\n",
    "**Pregunta 1: ¿Cómo abordaría este problema?**\n",
    "\n",
    "**Pregunta 2: ¿Importaría un modelo que incorpore la equidad?**\n",
    "\n",
    "**Pregunta 3: ¿Qué significa que nuestro modelo sea injusto?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Fuentes de inspiración del taller</h2>\n",
    "\n",
    "Literatura:\n",
    "\n",
    "Selbst, Andrew D. y Boyd, Danah y Friedler, Sorelle y Venkatasubramanian, Suresh y Vertesi, Janet, \"Equidad y abstracción en sistemas sociotécnicos\", (23 de agosto de 2018). Conferencia de ACM de 2019 sobre equidad, responsabilidad y transparencia (FAT *), 59-68, disponible en SSRN: https://ssrn.com/abstract=3265913,\n",
    "\n",
    "Tutorial de ciencia ficción de Fairlearn 2021:\n",
    "\n",
    "Tutorial de SciPy 2021: Equidad en los sistemas de IA: del contexto social a la práctica con Fairlearn por Manojit Nandi, Miroslav Dudík, Triveni Gandhi, Lisa Ibañez, Adrin Jalali, Michael Madaio, Hanna Wallach, Hilde Weerts tiene licencia CC BY 4.0._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Por qué es importante la ética en la IA</h2>\n",
    "\n",
    "Los sistemas de IA pueden comportarse injustamente por una variedad de razones:\n",
    "\n",
    "Los sesgos sociales se reflejan en los datos de formación.\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/bias.jpeg\" width=\"400\"/>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Por qué es importante la ética en la IA</h2>\n",
    "\n",
    "Los sistemas de IA pueden comportarse injustamente por una variedad de razones:\n",
    "\n",
    "2. Los sesgos sociales se reflejan en las decisiones que se toman durante el desarrollo y despliegue de estos sistemas.\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/development.jpeg\" width=\"500\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Por qué es importante la ética en la IA</h2>\n",
    "\n",
    "Los sistemas de IA pueden comportarse injustamente por una variedad de razones:\n",
    "\n",
    "3. Los sistemas de inteligencia artificial se comportan de manera injusta debido a características de los datos o características de los propios sistemas.\n",
    "\n",
    "$$\\Rightarrow \\text{No son mutuamente excluyentes y a menudo se exacerban entre sí.} \\Leftarrow$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Ejemplo</h2>\n",
    "\n",
    "Considere el uso de \"puntajes de riesgo\" para decidir los resultados que experimentarán las personas.\n",
    "\n",
    "Riesgo de reincidencia en la delincuencia.\n",
    "Riesgo de fracaso en un nuevo rol durante la contratación.\n",
    "Riesgo de fracaso en el tratamiento elegido por el paciente.\n",
    "\n",
    "⇒ ¿La partitura representa con precisión la realidad?\n",
    "\n",
    "⇒ ¿Cuál es el impacto de una asignación indebida?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>¿Cómo podemos determinar si una IA se está comportando de manera injusta?</h2>\n",
    "\n",
    "* Identifica conceptos erróneos subyacentes dentro de la IA:\n",
    "\n",
    "    1. Causal: contexto / sesgo basado en la sociedad \n",
    "    2. Basado en la intención: prejuicio \n",
    "\n",
    "* Mediante el estudio del impacto de la IA en las personas:\n",
    "\n",
    "    1. Daños\n",
    "    2. Ganancias\n",
    "\n",
    "**Para este taller, definimos si un sistema de IA se está comportando de manera injusta en términos de su impacto en las personas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Tipos de daños</h2>\n",
    "\n",
    "\n",
    "<b>Los daños en la asignación</b> pueden ocurrir cuando los sistemas de inteligencia artificial amplían o retienen oportunidades, recursos o información.\n",
    "\n",
    "Ejemplos de aplicaciones clave: contratación, admisiones escolares y préstamos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Tipos de daños</h2>\n",
    "\n",
    "<b>Los daños a la calidad del servicio</b> pueden ocurrir cuando un sistema no funciona tan bien para una persona como para otra, incluso si no se brindan o retienen oportunidades, recursos o información.\n",
    "\n",
    "Ejemplos de aplicaciones clave: precisión en el reconocimiento facial, búsqueda de documentos o recomendación de productos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>¿Cómo incurrimos en un daño?</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Cómo trabajamos con la información: abstracción</h2>\n",
    "<br>\n",
    "<center>\n",
    "Abstracción en ciencias de la computación\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "Abstracción en matemáticas\n",
    "</center>\n",
    "    \n",
    "<br>\n",
    "<center>\n",
    "Abstracción en el aprendizaje automático\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>¿Cómo falla la abstracción en el aprendizaje automático?</h2>\n",
    "\n",
    "\n",
    "⇒ La abstracción conduce a la eliminación de atributos o propiedades que dependen de un contexto social. Medir consequencias al remover o ignorar una propiedad.\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/noface.png\" width=\"400\"/>|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>¿Cómo falla la abstracción en el aprendizaje automático?</h2>\n",
    "\n",
    "⇒ ¿Cuál es la compensación que hacemos al desechar una propiedad y somos conscientes de las consecuencias de esa compensación?\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/tradeoff.jpeg\" width=\"500\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>¿Cómo falla la abstracción en el aprendizaje automático?</h2>\n",
    "\n",
    "⇒ ¿Podemos garantizar la capacidad para identificar y cuantificar las consecuencias de eliminar un atributo de base social?\n",
    "\n",
    "⇒ ¿Quién es el receptor de estas consecuencias?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Marco de referencia clave en la abstracción</h2>\n",
    "<h3 align='center'>El marco algorítmico</h3>\n",
    "\n",
    "Marco centrado en las elecciones tomadas al abstraer un problema en forma de representaciones (datos de entrada) y etiquetado (resultado).\n",
    "\n",
    "Precisión y generalización a los datos con los que el modelo no se entrenó.\n",
    "\n",
    "**La equidad no se puede definir en este marco ⇒ el objetivo es producir un modelo que capture mejor la relación entre las representaciones y las etiquetas.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Marco de referencia clave en la abstracción</h2>\n",
    "<h3 align='center'>El marco de datos</h3>\n",
    "\n",
    "Marco centrado en la calidad de los datos y los resultados. \n",
    "\n",
    "**Este marco adicional nos permite cuestionar la (des) imparcialidad inherente presente en los datos de entrada y salida.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Marco de referencia clave en la abstracción</h2>\n",
    "<h3 align='center'>El marco sociotécnico</h3>\n",
    "\n",
    "<center>Componente técnico + Componente social = Sistema sociotécnico</center>\n",
    "\n",
    "Reconoce que un modelo de aprendizaje automático es parte de la interacción entre las personas y la tecnología.\n",
    "\n",
    "Cualquier componente social de esta interacción debe modelarse e incorporarse en consecuencia.\n",
    "\n",
    "**Los diseñadores de sistemas de aprendizaje automático que no consideren la forma en que los contextos sociales y las tecnologías están interrelacionados corren el riesgo de caer en \"trampas de abstracción\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>¿En qué trampas podemos caer al modelar un problema social?</h2>\n",
    "\n",
    "Los sistemas de aprendizaje automático utilizados en el mundo real son intrínsecamente sistemas sociotécnicos.\n",
    "\n",
    "Los diseñadores de sistemas de aprendizaje automático suelen traducir un contexto del mundo real en un modelo de aprendizaje automático a través de la abstracción:\n",
    "\n",
    "* Aspectos 'relevantes' de ese contexto (entrada, salida, relación)\n",
    "\n",
    "* Los problemas surgen cuando abstraemos el contexto social."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>Trampas comunes en las que podemos caer</h2>\n",
    "\n",
    "Selbst et al. [1] identifican cinco trampas en las que podemos caer al implementar un modelo de aprendizaje automático:\n",
    "\n",
    "La trampa del solucionismo\n",
    "\n",
    "La trampa del efecto dominó\n",
    "\n",
    "La trampa del formalismo\n",
    "\n",
    "La trampa de la portabilidad\n",
    "\n",
    "La trampa del encuadre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>La trampa del solucionismo</h2>\n",
    "\n",
    "Esta trampa ocurre cuando asumimos que la mejor solución a un problema puede involucrar tecnología y no reconocemos otras posibles soluciones fuera de este ámbito. \n",
    "\n",
    "**Los enfoques solucionistas también fallan, ya que las definiciones de equidad se cuestionan a lo largo del tiempo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>La trampa del efecto dominó</h2>\n",
    "\n",
    "\n",
    "Esta trampa ocurre cuando no consideramos las consecuencias no deseadas de introducir tecnología en un sistema social existente.\n",
    "\n",
    "Cambios en los comportamientos, resultados, experiencias individuales o cambios en los valores e incentivos sociales subyacentes de un sistema social dado, por ejemplo, aumentando el valor percibido de las métricas cuantificables sobre las no cuantificables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>La trampa del formalismo</h2>\n",
    "\n",
    "Muchas tareas de un científico de datos implican alguna forma de formalización: desde medir fenómenos del mundo real como datos hasta traducir KPI y restricciones comerciales en métricas, funciones de pérdida o parámetros. Caemos en la trampa del formalismo cuando no damos cuenta del significado completo de conceptos sociales como la equidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>La trampa del formalismo</h2>\n",
    "\n",
    "\n",
    "La equidad es una construcción compleja que se cuestiona.\n",
    "\n",
    "Las métricas de equidad matemática pueden capturar algunos aspectos de la equidad, pero\n",
    "no captan todos los aspectos relevantes.\n",
    "\n",
    "Las métricas de equidad grupal no tienen en cuenta las diferencias en las experiencias individuales ni la justicia procesal.\n",
    "\n",
    "Otra área en la que la abstracción matemática encuentra una limitación es cuando\n",
    "capturar información sobre procedimentalidad, contextualidad y contestabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>La trampa de la portabilidad</h2>\n",
    "\n",
    "Esta trampa ocurre cuando no entendemos cómo la reutilización de un modelo o algoritmo que está diseñado para un contexto social específico puede no necesariamente aplicarse a un contexto social diferente. Reutilizar una solución algorítmica y no tener en cuenta las diferencias en los contextos sociales involucrados puede generar resultados engañosos y consecuencias potencialmente dañinas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>La trampa del marco de referencia</h2>\n",
    "\n",
    "Esta trampa ocurre cuando no consideramos la imagen completa que rodea un contexto social particular al abstraer un problema social.\n",
    "\n",
    "Paisaje social\n",
    "\n",
    "Características de los individuos o circunstancias de la situación elegida.\n",
    "\n",
    "Terceros involucrados junto con sus circunstancias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Introducción a Fairlearn:<br> una biblioteca de Python centrada en disminuir la injusticia en los modelos de aprendizaje automático </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Sobre Fairlearn</h2>\n",
    "\n",
    "\n",
    "Fairlearn es un proyecto de código abierto impulsado por la comunidad para ayudar a los científicos de datos a mejorar la equidad de los sistemas de IA. Incluye:\n",
    "\n",
    "Una biblioteca de Python para la evaluación y mejora de la equidad (métricas de equidad, algoritmos de mitigación, trazado, etc.)\n",
    "\n",
    "Recursos educativos que cubren procesos organizativos y técnicos para la mitigación de injusticias (guía de usuario, estudios de casos, cuadernos de Jupyter, etc.)\n",
    "\n",
    "El proyecto se inició en 2018 en Microsoft Research. En 2021 adoptó una estructura de gobernanza neutral y desde entonces está completamente impulsada por la comunidad.\n",
    "\n",
    "Leer más: https://fairlearn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Introducción al escenario asistencial</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Sección de códigos prácticos</h2>\n",
    "\n",
    "\n",
    "Los pasos clave en nuestro procesamiento serán (asumiendo que comenzamos con un conjunto de datos limpio):\n",
    "\n",
    "1. Determine la característica clave que usaremos para medir la equidad.\n",
    "\n",
    "2. Determina la validez predictiva.\n",
    "\n",
    "3. Identificar los desequilibrios presentes en los datos.\n",
    "\n",
    "4. Preparar datos de prueba y entrenamiento (sklearn).\n",
    "\n",
    "5. Modelo de tren (sklearn).\n",
    "\n",
    "6. Evalúe la precisión del modelo.\n",
    "\n",
    "7. Evalúe y ajuste la equidad del modelo con Fairlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%run -i process_health_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"race\"].value_counts(normalize=True).plot(kind='bar', rot=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts(normalize=True).plot(kind='bar', rot=45); # frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pointplot(df, \"race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"readmit_30_days\"].value_counts(normalize=True).plot(kind='bar') # frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imbalance en las clasificaciones \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's examine how much the label frequencies vary within each group defined by `race`:\n",
    "\n",
    "sns.barplot(x=\"readmit_30_days\", y=\"race\", data=df, ci=95);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparar conjuntos de datos de prueba y entrenamiento \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "[X_train, X_test, Y_train, Y_test, A_train, A_test, df_train, df_test, X] = train_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nuestra métrica de rendimiento es precisión equilibrada, por lo que para fines de capacitación (¡pero no de evaluación!) Volveremos a muestrear el conjunto de datos para que tenga la misma cantidad de ejemplos positivos y negativos. Esto significa que podemos usar estimadores que optimicen la precisión estándar (aunque algunos estimadores nos permiten usar pesos de importancia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_bal, Y_train_bal, A_train_bal = resample_dataset(X_train, Y_train, A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Guardando las estadísticas descriptivas de los datos de entrenamiento y prueba.\n",
    "\n",
    "A continuación, evaluamos y guardamos estadísticas descriptivas del conjunto de datos de entrenamiento. Estos se proporcionarán como parte de tarjetas modelo para documentar nuestra capacitación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_descriptive_stats(A_train_bal, Y_train_bal, A_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entrenando el modelo\n",
    "\n",
    "Entrenamos un modelo de regresión logística y guardamos sus predicciones en datos de prueba para su análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "unmitigated_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessing\", StandardScaler()),\n",
    "    (\"logistic_regression\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Fit data\n",
    "unmitigated_pipeline.fit(X_train_bal, Y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n",
    "Y_pred_proba = unmitigated_pipeline.predict_proba(X_test)[:,1]\n",
    "Y_pred = unmitigated_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve of probabilistic predictions\n",
    "plot_roc_curve(unmitigated_pipeline, X_test, Y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show balanced accuracy rate of the 0/1 predictions\n",
    "balanced_accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como vemos, el rendimiento del modelo está muy por encima del rendimiento de un lanzamiento de moneda (cuyo rendimiento sería de 0,5 en ambos casos), aunque está bastante lejos de un clasificador perfecto (cuyo rendimiento sería de 1,0 en ambos casos).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspeccionar los coeficientes del modelo entrenado\n",
    "\n",
    "Comprobamos los coeficientes del modelo ajustado para asegurarnos de que \"tienen sentido\". Si bien es subjetivo, este paso es importante y ayuda a detectar errores y puede señalar algunos problemas de equidad. Sin embargo, evaluaremos sistemáticamente la equidad del modelo en la siguiente sección.\n",
    "\n",
    "Los coeficientes representan las probabilidades logarítmicas del modelo. Duplicar una variable categórica, o aumentar una variable continua por una desviación estándar, duplica las probabilidades de que el clasificador vaya hacia el caso “1”. Esta es una verdadera medida estadística (según el modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coef_series = pd.Series(data=unmitigated_pipeline.named_steps[\"logistic_regression\"].coef_[0], index=X.columns)\n",
    "coef_series.sort_values().plot.barh(figsize=(4, 12), legend=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluación de equidad con MetricFrame\n",
    "\n",
    "Fairlearn proporciona la estructura de datos denominada MetricFrame para permitir la evaluación de métricas desagregadas. Mostraremos cómo usar un objeto MetricFrame para evaluar el clasificador entrenado LogisticRegression en busca de posibles daños relacionados con la equidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# In its simplest form MetricFrame takes four arguments:\n",
    "#    metric_function with signature metric_function(y_true, y_pred)\n",
    "#    y_true: array of labels\n",
    "#    y_pred: array of predictions\n",
    "#    sensitive_features: array of sensitive feature values\n",
    "\n",
    "mf1 = MetricFrame(metrics=false_negative_rate,\n",
    "                  y_true=Y_test,\n",
    "                  y_pred=Y_pred,\n",
    "                  sensitive_features=df_test['race'])\n",
    "\n",
    "# The disaggregated metrics are stored in a pandas Series mf1.by_group:\n",
    "\n",
    "mf1.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The largest difference, smallest ratio and worst-case performance are accessed as\n",
    "#   mf1.difference(), mf1.ratio(), mf1.group_max()\n",
    "\n",
    "print(f\"difference: {mf1.difference():.3}\\n\"\n",
    "      f\"ratio: {mf1.ratio():.3}\\n\"\n",
    "      f\"max across groups: {mf1.group_max():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# You can also evaluate multiple metrics by providing a dictionary\n",
    "\n",
    "metrics_dict = {\n",
    "    \"selection_rate\": selection_rate,\n",
    "    \"false_negative_rate\": false_negative_rate,\n",
    "    \"balanced_accuracy\": balanced_accuracy_score,\n",
    "}\n",
    "\n",
    "metricframe_unmitigated = MetricFrame(metrics=metrics_dict,\n",
    "                  y_true=Y_test,\n",
    "                  y_pred=Y_pred,\n",
    "                  sensitive_features=df_test['race'])\n",
    "\n",
    "# The disaggregated metrics are then stored in a pandas DataFrame:\n",
    "\n",
    "metricframe_unmitigated.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The largest difference, smallest ratio, and the maximum and minimum values\n",
    "# across the groups are then all pandas Series, for example:\n",
    "\n",
    "metricframe_unmitigated.difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# You'll probably want to view them transposed:\n",
    "pd.DataFrame({'difference': metricframe_unmitigated.difference(),\n",
    "              'ratio': metricframe_unmitigated.ratio(),\n",
    "              'group_min': metricframe_unmitigated.group_min(),\n",
    "              'group_max': metricframe_unmitigated.group_max()}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# You can also easily plot all of the metrics using DataFrame plotting capabilities\n",
    "\n",
    "metricframe_unmitigated.by_group.plot.bar(subplots=True, layout= [1,3], figsize=(12, 4),\n",
    "                      legend=False, rot=-45, position=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Según el gráfico de barras anterior, parece que el grupo Desconocido se selecciona para el programa de gestión de la atención con menos frecuencia que otros grupos, como lo refleja la tasa de selección. Además, este grupo experimenta la mayor tasa de falsos negativos, por lo que no se selecciona una fracción mayor de los miembros del grupo que probablemente se beneficiarán del programa de administración de la atención. Finalmente, la precisión equilibrada en este grupo también es la más baja.\n",
    "\n",
    "Observamos disparidad, aunque no incluimos la raza en nuestro modelo. Hay una variedad de razones por las que pueden ocurrir tales disparidades. Podría deberse a problemas de representación (es decir, no hay suficientes instancias por grupo), o porque la distribución de características en sí difiere entre los grupos (es decir, una relación diferente entre las características y la variable objetivo, un ejemplo obvio serían las personas con piel más oscura en los sistemas de reconocimiento facial, pero puede ser mucho más sutil). Las aplicaciones del mundo real a menudo presentan ambos tipos de problemas al mismo tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Postprocesamiento con ThresholdOptimizer\n",
    "\n",
    "Las técnicas de posprocesamiento son una clase de algoritmos de mitigación de injusticias que toman un modelo ya entrenado y un conjunto de datos como entrada y buscan ajustar una función de transformación a los resultados del modelo para satisfacer algunas restricciones de equidad (de grupo). Pueden ser el único enfoque factible de mitigación de la injusticia cuando los desarrolladores no pueden influir en la capacitación del modelo, debido a razones prácticas o debido a la seguridad o la privacidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aquí utilizamos el algoritmo ThresholdOptimizer de Fairlearn, que sigue el enfoque de Hardt, Price y Srebro (2016).\n",
    "\n",
    "ThresholdOptimizer toma un modelo de aprendizaje automático existente (posiblemente pre-ajustado) cuyas predicciones actúan como una función de puntuación e identifica un umbral separado para cada grupo con el fin de optimizar alguna métrica objetiva especificada (como la precisión equilibrada) sujeta a restricciones de equidad especificadas (como como paridad de tipos de falsos negativos). Por lo tanto, el clasificador resultante es solo una versión adecuadamente umbral del modelo de aprendizaje automático subyacente.\n",
    "\n",
    "La restricción de paridad de tasas de falsos negativos requiere que todos los grupos tengan valores iguales de tasa de falsos negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para instalar nuestro `ThresholdOptimizer`, pasamos:\n",
    "\n",
    "* Un \"estimador\" existente que deseamos establecer como umbral.\n",
    "* Las \"limitaciones\" de equidad que queremos satisfacer.\n",
    "* La métrica del \"objetivo\" que queremos maximizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now we instantite ThresholdOptimizer with the logistic regression estimator\n",
    "postprocess_est = ThresholdOptimizer(\n",
    "    estimator=unmitigated_pipeline,\n",
    "    constraints=\"false_negative_rate_parity\",\n",
    "    objective=\"balanced_accuracy_score\",\n",
    "    prefit=True,\n",
    "    predict_method='predict_proba'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para usar el `ThresholdOptimizer`, necesitamos acceso a las funciones sensibles ** tanto durante el tiempo de entrenamiento como una vez implementado **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "postprocess_est.fit(X_train_bal, Y_train_bal, sensitive_features=A_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Record and evaluate the output of the trained ThresholdOptimizer on test data\n",
    "\n",
    "Y_pred_postprocess = postprocess_est.predict(X_test, sensitive_features=A_test)\n",
    "metricframe_postprocess = MetricFrame(\n",
    "    metrics=metrics_dict,\n",
    "    y_true=Y_test,\n",
    "    y_pred=Y_pred_postprocess,\n",
    "    sensitive_features=A_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([metricframe_unmitigated.by_group,\n",
    "           metricframe_postprocess.by_group],\n",
    "           keys=['Unmitigated', 'ThresholdOptimizer'],\n",
    "           axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([metricframe_unmitigated.difference(),\n",
    "           metricframe_postprocess.difference()],\n",
    "          keys=['Unmitigated: difference', 'ThresholdOptimizer: difference'],\n",
    "          axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "metricframe_postprocess.by_group.plot.bar(subplots=True, layout=[1,3], figsize=(12, 4), legend=False, rot=-45, position=1.5)\n",
    "postprocess_performance = figure_to_base64str(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Observaciones finales</h2>\n",
    "\n",
    "\n",
    "En este taller motivamos la importancia de incorporar y evaluar métricas de equidad como parte de un problema que utiliza técnicas de aprendizaje automático.\n",
    "\n",
    "Cubrimos una descripción general de qué es la abstracción, cómo ocurren los daños en la abstracción y las diferentes trampas en las que podemos caer cuando nuestra abstracción no incorpora el contexto social.\n",
    "\n",
    "Aplicamos lo que aprendimos a un escenario que involucraba la inscripción de pacientes en un programa de administración de atención de alto riesgo. Exploramos la biblioteca Fairlearn Python para mejorar los resultados injustos hacia los pacientes cuya raza era desconocida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecturas y ejercicios futuros:\n",
    "\n",
    "Puede obtener más información sobre Fairlearn https://fairlearn.org/v0.7.0/about/index.html y visitar su código fuente abierto https://github.com/fairlearn/fairlearn\n",
    "\n",
    "Fairlearn es un esfuerzo comunitario para mejorar la ética en IA.\n",
    "\n",
    "¿Tienes curiosidad por contribuir? Visite https://fairlearn.org/v0.7.0/contributor_guide/index.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "rise": {
   "autolaunch": false,
   "backimage": "Cybera_contact.svg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
